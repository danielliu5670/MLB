{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9BxRoZEJ3-56",
        "7CsqDJSR4FtR",
        "5nAwOLSX6a7H"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boring package stuff:\n",
        "As a note, I'm storing my Iris.csv file in my Google Drive, so if you're doing that to test the code, you might have to change the file path I have in the test code below."
      ],
      "metadata": {
        "id": "9BxRoZEJ3-56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m48o62cVOSU",
        "outputId": "12eca2e4-5d69-4ce5-8418-ef3dc84d67b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datascience in /usr/local/lib/python3.10/dist-packages (0.17.6)\n",
            "Requirement already satisfied: folium>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from datascience) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from datascience) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datascience) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datascience) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from datascience) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from datascience) (1.26.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from datascience) (7.34.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from datascience) (5.24.1)\n",
            "Requirement already satisfied: branca in /usr/local/lib/python3.10/dist-packages (from datascience) (0.8.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium>=0.9.1->datascience) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium>=0.9.1->datascience) (2.32.3)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.10/dist-packages (from folium>=0.9.1->datascience) (2024.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->datascience) (2.8.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->datascience) (4.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datascience) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datascience) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->datascience) (9.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->datascience) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium>=0.9.1->datascience) (2.1.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->datascience) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->datascience) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->datascience) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->folium>=0.9.1->datascience) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->folium>=0.9.1->datascience) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->folium>=0.9.1->datascience) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->folium>=0.9.1->datascience) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install datascience"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datascience import *"
      ],
      "metadata": {
        "id": "2rV1DbNWXdLO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3fcTQ_aXtZ_",
        "outputId": "8fd464cb-5c79-4862-b531-dfbe3aac1efe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual code:"
      ],
      "metadata": {
        "id": "7CsqDJSR4FtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this cell creates the framework for the actual nodes that make up the decision tree."
      ],
      "metadata": {
        "id": "QVWa1tVNEZ01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "   def __init__(self, value):\n",
        "      self.left = None\n",
        "      self.right = None\n",
        "      self.value = value\n",
        "\n",
        "   def insert(self, value):\n",
        "      if self.value:\n",
        "          if self.left is None:\n",
        "              self.left = Node(value)\n",
        "          elif self.right is None:\n",
        "              self.right = Node(value)\n",
        "          elif self.left.left is not None and self.left.right is not None:\n",
        "              self.right.insert(value)\n",
        "          else:\n",
        "              self.left.insert(value)\n",
        "\n",
        "   def insertleft(self, value):\n",
        "      if self.left is None:\n",
        "          self.left = Node(value)\n",
        "      else:\n",
        "          self.left.insertleft(value)\n",
        "\n",
        "   def insertright(self, value):\n",
        "      if self.right is None:\n",
        "          self.right = Node(value)\n",
        "      else:\n",
        "          self.right.insertright(value)\n",
        "\n",
        "   def cleartree(self):\n",
        "      self.value = None\n",
        "      self.left = None\n",
        "      self.right = None\n",
        "\n",
        "   def printtree(self):\n",
        "      print(self.value)\n",
        "      if self.left:\n",
        "          self.left.printtree()\n",
        "      if self.right:\n",
        "          self.right.printtree()"
      ],
      "metadata": {
        "id": "Wiv_kW0-_KYk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this cell does all of the tedious calculation, like Gini impurity and tree construction."
      ],
      "metadata": {
        "id": "guqNhP_0Ed6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, dataset_train, dataset_test, class_column_name):\n",
        "        self.dataset_train = dataset_train\n",
        "        self.dataset_test = dataset_test\n",
        "        self.root_node = Node(0)\n",
        "        self.class_column_name = class_column_name\n",
        "\n",
        "    # given a list of classifications, generates the gini impurity:\n",
        "    #   ex: ['apple', 'orange', 'orange', 'banana']\n",
        "\n",
        "    def gini_impurity(self, classification_array):\n",
        "        classes_lst = {}\n",
        "        purity = 0\n",
        "        for element in classification_array:\n",
        "            if element not in classes_lst:\n",
        "                  classes_lst[element] = classification_array.count(element)\n",
        "        for category in classes_lst:\n",
        "            category_count = classes_lst[category]\n",
        "            purity += ((category_count / len(classification_array)) ** 2)\n",
        "        return 1 - purity\n",
        "\n",
        "    # generates an \"in-between column\" of data composed of split points:\n",
        "    #   ex: [5,    6,    7,    8,    9] -->\n",
        "    #           ^     ^     ^     ^\n",
        "    #         [5.5,  6.5,  7.5,  8.5]\n",
        "\n",
        "    def column_splitter(self, column_lst):\n",
        "        column_splitter_lst = []\n",
        "        for i in range(0, len(column_lst) - 1):\n",
        "            average = (column_lst[i] + column_lst[i + 1]) / 2\n",
        "            column_splitter_lst.append(average)\n",
        "        return column_splitter_lst\n",
        "\n",
        "    # generates a list of weighted gini impurity values associated with each split point mentioned above:\n",
        "    # ex: [[5.5, 0.4], [6.5, 0.83], [7.5, 0.2], [8.5, 0.9]]\n",
        "    #.       ^ [0] = split_point           ^ [1] = weighted gini impurity\n",
        "\n",
        "    def column_partitioner_impurity_calc(self, column_splitter_lst, table, column_name, class_column):\n",
        "        impurity_lst = []\n",
        "        less_than_lst = []\n",
        "        more_than_lst = []\n",
        "        for row_num in range(len(column_splitter_lst)):\n",
        "            current_split_point = column_splitter_lst[row_num]\n",
        "\n",
        "            less_than_lst = table.where(column_name, are.below(current_split_point)).column(class_column).tolist()\n",
        "            more_than_lst = table.where(column_name, are.above(current_split_point)).column(class_column).tolist()\n",
        "\n",
        "            less_than_gini = self.gini_impurity(less_than_lst)\n",
        "            more_than_gini = self.gini_impurity(more_than_lst)\n",
        "\n",
        "            weighted_split_gini = ((len(less_than_lst) / table.num_rows) * less_than_gini) + ((len(more_than_lst) / table.num_rows) * more_than_gini)\n",
        "\n",
        "            impurity_lst.append([current_split_point, weighted_split_gini])\n",
        "        return impurity_lst\n",
        "\n",
        "    # calculates the minimum gini impurity for the entire column out of all the split points,\n",
        "    # with the associated split point and column name\n",
        "\n",
        "    def gini_for_whole_columns(self, table):\n",
        "        columns_with_ginis = []\n",
        "        for label in table.labels[:len(table.labels) - 1]:\n",
        "            min_gini_for_column = 1.0\n",
        "            split_point = None\n",
        "            impurity_lst = self.column_partitioner_impurity_calc(self.column_splitter(table.column(label).tolist()), table, label, self.class_column_name)\n",
        "            for i in range(len(impurity_lst)):\n",
        "                if impurity_lst[i][1] < min_gini_for_column:\n",
        "                    min_gini_for_column = impurity_lst[i][1]\n",
        "                    split_point = impurity_lst[i][0]\n",
        "            columns_with_ginis.append([label, split_point, min_gini_for_column])\n",
        "        return columns_with_ginis\n",
        "\n",
        "    # calculates which split point from which column returns the lowest gini impurity out of all of the columns\n",
        "    #   ex: would help calculate which should be the head node initially, then the rest of the nodes as well\n",
        "\n",
        "    def minimum_gini(self, columns_with_ginis):\n",
        "        min_gini = 1.0\n",
        "        min_split_point = None\n",
        "        min_column = None\n",
        "        for i in range(len(columns_with_ginis)):\n",
        "            if columns_with_ginis[i][2] < min_gini:\n",
        "                min_gini = columns_with_ginis[i][2]\n",
        "                min_split_point = columns_with_ginis[i][1]\n",
        "                min_column = columns_with_ginis[i][0]\n",
        "        return [min_column, min_split_point, min_gini]\n",
        "\n",
        "    # splits table into two, based on the associated split_point.\n",
        "\n",
        "    def split_data(self, table, split_point, column):\n",
        "        less_than_tbl = table.where(column, are.below(split_point))\n",
        "        more_than_tbl = table.where(column, are.above(split_point))\n",
        "        return [less_than_tbl, more_than_tbl]\n",
        "\n",
        "    # constructs the actual decision tree\n",
        "\n",
        "    def construct_tree(self, table, node):\n",
        "        if node is not None and isinstance(node.value, list) and table.num_rows > 1:\n",
        "            temp_split_point = self.minimum_gini(self.gini_for_whole_columns(table))[1]\n",
        "            temp_column = self.minimum_gini(self.gini_for_whole_columns(table))[0]\n",
        "            decision = self.split_data(table, temp_split_point, temp_column)\n",
        "\n",
        "            if decision[0] is not None:\n",
        "                sum = 0\n",
        "                for i in self.gini_for_whole_columns(decision[0]):\n",
        "                    sum += i[2]\n",
        "                if round(sum, 3) > 0:\n",
        "                    node.insertleft([self.minimum_gini(self.gini_for_whole_columns(decision[0]))[1], self.minimum_gini(self.gini_for_whole_columns(decision[0]))[0]])\n",
        "                    self.construct_tree(decision[0], node.left)\n",
        "                elif round(sum, 3) == 0:\n",
        "                    node.insertleft(decision[0].column(self.class_column_name).item(0))\n",
        "\n",
        "            if decision[1] is not None:\n",
        "                sum = 0\n",
        "                for i in self.gini_for_whole_columns(decision[1]):\n",
        "                    sum += i[2]\n",
        "                if round(sum, 3) > 0:\n",
        "                    node.insertright([self.minimum_gini(self.gini_for_whole_columns(decision[1]))[1], self.minimum_gini(self.gini_for_whole_columns(decision[1]))[0]])\n",
        "                    self.construct_tree(decision[1], node.right)\n",
        "                elif round(sum, 3) == 0:\n",
        "                    node.insertright(decision[1].column(self.class_column_name).item(0))\n",
        "        elif node is None:\n",
        "            print(\"?\")\n",
        "        elif table.num_rows == 0:\n",
        "            node.value = \"No data\"\n",
        "        elif table.num_rows == 1:\n",
        "            node.value = table.column(self.class_column_name).item(0)\n",
        "\n",
        "    # first clears the tree, then creates a root node and its two first initial branches,\n",
        "    # and then runs the tree construction on each of the branches. Then, it runs the tree.\n",
        "\n",
        "    def tree_initialization(self):\n",
        "\n",
        "        columns_with_ginis = self.gini_for_whole_columns(self.dataset_train)\n",
        "        min_column = self.minimum_gini(columns_with_ginis)[0]\n",
        "        min_split_point = round(self.minimum_gini(columns_with_ginis)[1], 3)\n",
        "        min_gini = self.minimum_gini(columns_with_ginis)[2]\n",
        "\n",
        "        self.root_node.cleartree()\n",
        "        self.root_node = Node([min_split_point, min_column])\n",
        "\n",
        "        split = self.split_data(self.dataset_train, min_split_point, min_column)\n",
        "        min_split_point_left = self.minimum_gini(self.gini_for_whole_columns(split[0]))[1]\n",
        "        min_split_point_right = self.minimum_gini(self.gini_for_whole_columns(split[1]))[1]\n",
        "        min_column_left = self.minimum_gini(self.gini_for_whole_columns(split[0]))[0]\n",
        "        min_column_right = self.minimum_gini(self.gini_for_whole_columns(split[1]))[0]\n",
        "\n",
        "        self.root_node.insertleft([min_split_point_left, min_column_left])\n",
        "        self.root_node.insertright([min_split_point_right, min_column_right])\n",
        "\n",
        "        self.construct_tree(split[0], self.root_node.left)\n",
        "        self.construct_tree(split[1], self.root_node.right)\n",
        "\n",
        "\n",
        "    # given a list of attributes (only the values) and the species, gives whether or not\n",
        "    # the prediction was correct given the attributes.\n",
        "\n",
        "    def predict_once(self, attributes_lst, table, correct_or_predict):\n",
        "        attributes_dct = {}\n",
        "        labels_lst = table.labels\n",
        "        for i in range(len(attributes_lst)):\n",
        "            attributes_dct[labels_lst[i]] = attributes_lst[i]\n",
        "        ref_node = self.root_node\n",
        "        ref_node.value = self.root_node.value\n",
        "        while not isinstance(ref_node.value, str):\n",
        "            if round(attributes_dct[ref_node.value[1]], 3) < ref_node.value[0]:\n",
        "                ref_node = ref_node.left\n",
        "            else:\n",
        "                ref_node = ref_node.right\n",
        "        if correct_or_predict == \"correct\":\n",
        "            return ref_node.value == attributes_dct[self.class_column_name]\n",
        "        elif correct_or_predict == \"predict\":\n",
        "            return ref_node.value\n",
        "        else:\n",
        "            return \"please enter \\\"correct\\\" or \\\"predict\\\"\"\n",
        "\n",
        "    # calculates the accuracy of the testdata predictions\n",
        "\n",
        "    def accuracy(self):\n",
        "        count_correct = 0\n",
        "        for i in range(len(self.dataset_test.values)):\n",
        "            if self.predict_once(self.dataset_test.values[i].tolist(), self.dataset_test, \"correct\"):\n",
        "                count_correct += 1\n",
        "\n",
        "        return [count_correct / self.dataset_test.num_rows, count_correct]"
      ],
      "metadata": {
        "id": "GuO5BiECX-17"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing:"
      ],
      "metadata": {
        "id": "5nAwOLSX6a7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Always run this first cell:"
      ],
      "metadata": {
        "id": "7fhnvjlH7Sch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/My Drive/MLB_2024_2025/Iris.csv\" #change this to wherever you have the file\n",
        "iris_data = Table.read_table(path)"
      ],
      "metadata": {
        "id": "urX4QR0Ncfhz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this one if you want to see which combination of test and train is most accurate for that specific shuffled dataset."
      ],
      "metadata": {
        "id": "RwOy4Kzc7T0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"**repetitions**\" refers to how many times you want that specific train/test split\n",
        "to run, and then return the averaged out results.\n",
        "\n",
        "\"**min_train**\" refers to the minimum number of training rows you want when the\n",
        "code is iterating through, max_train is the maximum you want.\n",
        "\n",
        "\"**num_rows**\" refers to the number of rows in the dataset you're running the code on."
      ],
      "metadata": {
        "id": "GUyutPU9DZ5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repetitions = 5\n",
        "min_train = 15\n",
        "max_train = 135\n",
        "num_rows = 150\n",
        "\n",
        "sorting = {}\n",
        "for iter in range(min_train, max_train + 1):\n",
        "    accuracies = []\n",
        "    for i in range(repetitions):\n",
        "        iris_data = iris_data.shuffle()\n",
        "        relevant_data = iris_data.take(range(iter)).drop(\"Id\") # dropping unnecessary columns\n",
        "        testdata = iris_data.take(range(iter, num_rows)).drop(\"Id\")\n",
        "        tree = DecisionTree(relevant_data, testdata, \"Species\") # third argument is the name of the class column\n",
        "        tree.tree_initialization()\n",
        "        accuracy = tree.accuracy()[0]\n",
        "        accuracies.append(accuracy)\n",
        "    test = 150 - iter\n",
        "    overall_accuracy = (round(sum(accuracies) / len(accuracies), 3))\n",
        "    sorting[overall_accuracy] = (\"train: \" + str(iter) + \", test: \" + str(test) + \"\\n accuracy: \" + str(overall_accuracy))\n",
        "\n",
        "sorting_keys = list(sorting.keys())\n",
        "sorting_keys.sort()\n",
        "sorting_keys.reverse()\n",
        "for i in sorting_keys:\n",
        "    print(sorting[i] + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqsFlA2p7MXX",
        "outputId": "1122a236-e0a0-4d9d-e856-ee61667468d5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 107, test: 43\n",
            " accuracy: 0.972\n",
            "\n",
            "train: 130, test: 20\n",
            " accuracy: 0.97\n",
            "\n",
            "train: 135, test: 15\n",
            " accuracy: 0.96\n",
            "\n",
            "train: 67, test: 83\n",
            " accuracy: 0.959\n",
            "\n",
            "train: 99, test: 51\n",
            " accuracy: 0.957\n",
            "\n",
            "train: 132, test: 18\n",
            " accuracy: 0.956\n",
            "\n",
            "train: 93, test: 57\n",
            " accuracy: 0.954\n",
            "\n",
            "train: 44, test: 106\n",
            " accuracy: 0.953\n",
            "\n",
            "train: 58, test: 92\n",
            " accuracy: 0.952\n",
            "\n",
            "train: 65, test: 85\n",
            " accuracy: 0.951\n",
            "\n",
            "train: 78, test: 72\n",
            " accuracy: 0.95\n",
            "\n",
            "train: 91, test: 59\n",
            " accuracy: 0.949\n",
            "\n",
            "train: 123, test: 27\n",
            " accuracy: 0.948\n",
            "\n",
            "train: 97, test: 53\n",
            " accuracy: 0.947\n",
            "\n",
            "train: 50, test: 100\n",
            " accuracy: 0.946\n",
            "\n",
            "train: 125, test: 25\n",
            " accuracy: 0.944\n",
            "\n",
            "train: 129, test: 21\n",
            " accuracy: 0.943\n",
            "\n",
            "train: 119, test: 31\n",
            " accuracy: 0.942\n",
            "\n",
            "train: 133, test: 17\n",
            " accuracy: 0.941\n",
            "\n",
            "train: 64, test: 86\n",
            " accuracy: 0.94\n",
            "\n",
            "train: 114, test: 36\n",
            " accuracy: 0.939\n",
            "\n",
            "train: 124, test: 26\n",
            " accuracy: 0.938\n",
            "\n",
            "train: 115, test: 35\n",
            " accuracy: 0.937\n",
            "\n",
            "train: 103, test: 47\n",
            " accuracy: 0.936\n",
            "\n",
            "train: 113, test: 37\n",
            " accuracy: 0.935\n",
            "\n",
            "train: 92, test: 58\n",
            " accuracy: 0.934\n",
            "\n",
            "train: 126, test: 24\n",
            " accuracy: 0.933\n",
            "\n",
            "train: 88, test: 62\n",
            " accuracy: 0.932\n",
            "\n",
            "train: 121, test: 29\n",
            " accuracy: 0.931\n",
            "\n",
            "train: 110, test: 40\n",
            " accuracy: 0.93\n",
            "\n",
            "train: 116, test: 34\n",
            " accuracy: 0.929\n",
            "\n",
            "train: 120, test: 30\n",
            " accuracy: 0.927\n",
            "\n",
            "train: 104, test: 46\n",
            " accuracy: 0.926\n",
            "\n",
            "train: 86, test: 64\n",
            " accuracy: 0.925\n",
            "\n",
            "train: 84, test: 66\n",
            " accuracy: 0.924\n",
            "\n",
            "train: 109, test: 41\n",
            " accuracy: 0.922\n",
            "\n",
            "train: 112, test: 38\n",
            " accuracy: 0.921\n",
            "\n",
            "train: 68, test: 82\n",
            " accuracy: 0.92\n",
            "\n",
            "train: 98, test: 52\n",
            " accuracy: 0.919\n",
            "\n",
            "train: 80, test: 70\n",
            " accuracy: 0.917\n",
            "\n",
            "train: 105, test: 45\n",
            " accuracy: 0.916\n",
            "\n",
            "train: 96, test: 54\n",
            " accuracy: 0.915\n",
            "\n",
            "train: 106, test: 44\n",
            " accuracy: 0.914\n",
            "\n",
            "train: 134, test: 16\n",
            " accuracy: 0.912\n",
            "\n",
            "train: 128, test: 22\n",
            " accuracy: 0.909\n",
            "\n",
            "train: 122, test: 28\n",
            " accuracy: 0.907\n",
            "\n",
            "train: 69, test: 81\n",
            " accuracy: 0.906\n",
            "\n",
            "train: 102, test: 48\n",
            " accuracy: 0.904\n",
            "\n",
            "train: 82, test: 68\n",
            " accuracy: 0.903\n",
            "\n",
            "train: 37, test: 113\n",
            " accuracy: 0.901\n",
            "\n",
            "train: 72, test: 78\n",
            " accuracy: 0.9\n",
            "\n",
            "train: 33, test: 117\n",
            " accuracy: 0.899\n",
            "\n",
            "train: 117, test: 33\n",
            " accuracy: 0.897\n",
            "\n",
            "train: 127, test: 23\n",
            " accuracy: 0.896\n",
            "\n",
            "train: 22, test: 128\n",
            " accuracy: 0.887\n",
            "\n",
            "train: 131, test: 19\n",
            " accuracy: 0.884\n",
            "\n",
            "train: 49, test: 101\n",
            " accuracy: 0.883\n",
            "\n",
            "train: 27, test: 123\n",
            " accuracy: 0.88\n",
            "\n",
            "train: 16, test: 134\n",
            " accuracy: 0.878\n",
            "\n",
            "train: 26, test: 124\n",
            " accuracy: 0.873\n",
            "\n",
            "train: 45, test: 105\n",
            " accuracy: 0.872\n",
            "\n",
            "train: 18, test: 132\n",
            " accuracy: 0.858\n",
            "\n",
            "train: 25, test: 125\n",
            " accuracy: 0.856\n",
            "\n",
            "train: 36, test: 114\n",
            " accuracy: 0.832\n",
            "\n",
            "train: 17, test: 133\n",
            " accuracy: 0.824\n",
            "\n",
            "train: 20, test: 130\n",
            " accuracy: 0.812\n",
            "\n",
            "train: 21, test: 129\n",
            " accuracy: 0.808\n",
            "\n",
            "train: 28, test: 122\n",
            " accuracy: 0.807\n",
            "\n",
            "train: 23, test: 127\n",
            " accuracy: 0.8\n",
            "\n",
            "train: 19, test: 131\n",
            " accuracy: 0.727\n",
            "\n",
            "train: 15, test: 135\n",
            " accuracy: 0.711\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this one if you want to just see one single test with shuffled data, it also gives you some customization of what train or test split you want."
      ],
      "metadata": {
        "id": "14inOrVM9uMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"**repetitions**\" refers to how many times you want that specific train/test split\n",
        "to run, and then return the averaged out results.\n",
        "\n",
        "\"**train**\" refers to the number of training rows you want when the code is iterating through.\n",
        "\n",
        "\"**num_rows**\" refers to the number of rows in the dataset you're running the code\n",
        "on."
      ],
      "metadata": {
        "id": "09Ysxa85Dg1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repetitions = 5\n",
        "train = 135\n",
        "num_rows = 150\n",
        "\n",
        "sorting = {}\n",
        "accuracies = []\n",
        "for i in range(repetitions):\n",
        "    iris_data = iris_data.shuffle()\n",
        "    relevant_data = iris_data.take(range(train)).drop(\"Id\")\n",
        "    testdata = iris_data.take(range(train, num_rows)).drop(\"Id\")\n",
        "    tree = DecisionTree(relevant_data, testdata, \"Species\")\n",
        "    tree.tree_initialization()\n",
        "    accuracy = tree.accuracy()[0]\n",
        "    accuracies.append(accuracy)\n",
        "overall_accuracy = (round(sum(accuracies) / len(accuracies), 3))\n",
        "print(\"accuracy: \" + str(overall_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyp16RCR7a7W",
        "outputId": "9604e0aa-0774-4751-b5f9-a2a9dc0246ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.933\n"
          ]
        }
      ]
    }
  ]
}